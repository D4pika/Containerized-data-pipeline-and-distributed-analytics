# Containerized-data-pipeline-and-distributed-analytics
Whitepaper (opinionated)
Data Processing in an organization consists of multiple steps beginning from extracting data from various systems that can require many steps (from copying data, to moving it from an on-premise location into the cloud) to reformatting it or joining it with other data sources, preprocessing it (for reduction of unwanted features and extracting of important ones), building inferential and predictive models, and post-processing of the prediction or inferences from these models or simply extracting insights. Each of these steps needs to be done in a timely manner, and usually requires separate softwareâ€™s.
A data pipeline is the sum of all these steps working together in a synchronous or non-synchronous manner, and its job is to ensure that these steps are executed reliably to all data. These processes must be automated, updated as per the need of a business with a mechanism for failure revival. In production, a pipeline, also known as a data pipeline, is a set of data processing elements interlinked with each other, where the output of one or more elements is the input of the next one or more. The elements of a pipeline at the same stage are often executed in parallel or in time-sliced fashion

A multi-staged data pipeline is needed to run analytics in production, and a conceptual architecture of pipeline must be defined, for parallel processing, modularity, isolation, reproducibility, individual scalability, performance and synchronized flow of data. 
